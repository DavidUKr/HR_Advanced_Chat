{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b17d220-68c1-4a97-96c1-bfdcc482544d",
   "metadata": {},
   "source": [
    "# csv transformare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a84e6225-d1d3-4fc0-ba8d-2352a058c58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: extracted_images\\Image1.jpg\n",
      "Image extraction complete.\n",
      "Page 1, Table 1 headers: ['region_id', 'region_name']\n",
      "Saved table: extracted_content\\Regions.csv, OK: 1\n",
      "Page 1, Table 2 headers: ['country_id', 'country_name', 'region_id']\n",
      "Saved table: extracted_content\\Countries.csv, OK: 1\n",
      "Page 2, Table 1 headers: ['location_id', 'street_address', 'postal_code', 'city', 'state_province', 'country_id']\n",
      "Saved table: extracted_content\\Locations.csv, OK: 1\n",
      "Page 3, Table 1 headers: ['department_id', 'department_name', 'manager_id', 'location_id']\n",
      "Saved table: extracted_content\\Departments.csv, OK: 1\n",
      "Page 4, Table 1 headers: ['job_id', 'job_title', 'min_salary', 'max_salary']\n",
      "Saved table: extracted_content\\Jobs.csv, OK: 1\n",
      "Page 5, Table 1 headers: ['employee_id', 'first_name', 'last_name', 'email', 'phone_number', 'hire_date', 'job_id', 'salary', 'commission_pct', 'manager_id', 'department_id']\n",
      "Saved table: extracted_content\\Employees.csv, OK: 1\n",
      "Page 6, Table 1 headers: ['132', 'TJ', 'Olson', 'TJOLSON', '650.124.8234', '10-APR-1999', 'ST_CLERK', '2100', 'NULL', '121', '50']\n",
      "Saved table: extracted_content\\Employees.csv, OK: 0\n",
      "Page 7, Table 1 headers: ['179', 'Charles', 'Johnson', 'CJOHNSON', '011.44.1644.429262', '04-JAN-2000', 'SA_REP', '6200', '.10', '149', '80']\n",
      "Saved table: extracted_content\\Employees.csv, OK: 0\n",
      "Table extraction complete.\n",
      "  country_id              country_name  region_id\n",
      "0         IT                     Italy          1\n",
      "1         JP                     Japan          3\n",
      "2         US  United States of America          2\n",
      "3         CA                    Canada          2\n",
      "4         CN                     China          3\n",
      "   department_id  department_name  manager_id  location_id\n",
      "0             10   Administration       200.0         1700\n",
      "1             20        Marketing       201.0         1800\n",
      "2             30       Purchasing       114.0         1700\n",
      "3             40  Human Resources       203.0         2400\n",
      "4             50         Shipping       121.0         1500\n",
      "   employee_id first_name last_name     email  phone_number    hire_date  \\\n",
      "0          100     Steven      King     SKING  515.123.4567  17-JUN-1987   \n",
      "1          101      Neena   Kochhar  NKOCHHAR  515.123.4568  21-SEP-1989   \n",
      "2          102        Lex   De Haan   LDEHAAN  515.123.4569  13-JAN-1993   \n",
      "3          103  Alexander    Hunold   AHUNOLD  590.423.4567  03-JAN-1990   \n",
      "4          104      Bruce     Ernst    BERNST  590.423.4568  21-MAY-1991   \n",
      "\n",
      "    job_id  salary  commission_pct  manager_id  department_id  \n",
      "0  AD_PRES   24000             NaN         NaN           90.0  \n",
      "1    AD_VP   17000             NaN       100.0           90.0  \n",
      "2    AD_VP   17000             NaN       100.0           90.0  \n",
      "3  IT_PROG    9000             NaN       102.0           60.0  \n",
      "4  IT_PROG    6000             NaN       103.0           60.0  \n",
      "       job_id                      job_title  min_salary  max_salary\n",
      "0     AD_PRES                      President       20000       40000\n",
      "1       AD_VP  Administration Vice President       15000       30000\n",
      "2     AD_ASST       Administration Assistant        3000        6000\n",
      "3      FI_MGR                Finance Manager        8200       16000\n",
      "4  FI_ACCOUNT                     Accountant        4200        9000\n",
      "   location_id           street_address postal_code       city  \\\n",
      "0         1000     1297 Via Cola di Rie       00989       Roma   \n",
      "1         1100  93091 Calle della Testa       10934     Venice   \n",
      "2         1200         2017 Shinjuku-ku        1689      Tokyo   \n",
      "3         1300          9450 Kamiya-cho        6823  Hiroshima   \n",
      "4         1400      2014 Jabberwocky Rd       26192  Southlake   \n",
      "\n",
      "     state_province country_id  \n",
      "0               NaN         IT  \n",
      "1               NaN         IT  \n",
      "2  Tokyo Prefecture         JP  \n",
      "3               NaN         JP  \n",
      "4             Texas         US  \n",
      "   region_id             region_name\n",
      "0          1                  Europe\n",
      "1          2                Americas\n",
      "2          3                    Asia\n",
      "3          4  Middle East and Africa\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def extract_table_titles(pdf_path):\n",
    "    # Deschide PDF-ul\n",
    "    doc = fitz.open(pdf_path)\n",
    "    table_titles = []\n",
    "    title_frequencies = defaultdict(int)\n",
    "\n",
    "    # Variabilă pentru a ține evidența rândurilor goale între titlurile de tabele\n",
    "    blank_lines_count = 0\n",
    "    \n",
    "    # Parcurge fiecare pagină\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            \n",
    "            # Verifică dacă linia este goală\n",
    "            if len(words) == 0:\n",
    "                blank_lines_count += 1\n",
    "            else:\n",
    "                # Verifică dacă linia conține un singur cuvânt care începe cu literă mare\n",
    "                if len(words) == 1 and words[0][0].isupper():\n",
    "                    # Pentru primul titlu de tabel, nu este nevoie să verificăm numărul de rânduri goale\n",
    "                    if not table_titles or blank_lines_count >= 2:\n",
    "                        table_titles.append(words[0])\n",
    "                        title_frequencies[words[0]] = 0\n",
    "                    blank_lines_count = 0  # Resetează contorul de rânduri goale\n",
    "                else:\n",
    "                    # Resetează contorul de rânduri goale dacă întâlnește o linie care nu este goală sau nu este titlu de tabel\n",
    "                    blank_lines_count = 0\n",
    "\n",
    "    return table_titles, dict(title_frequencies)\n",
    "    \n",
    "def extract_images_from_pdf(pdf_path, output_folder):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    index=0\n",
    "    \n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        image_list = page.get_images(full=True)\n",
    "\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image_path = os.path.join(output_folder,f\"Image{index + 1}.jpg\")\n",
    "            index=index+1\n",
    "            image.save(image_path)\n",
    "\n",
    "            print(f\"Saved image: {image_path}\")\n",
    "            \n",
    "\n",
    "    print(\"Image extraction complete.\")\n",
    "\n",
    "\n",
    "def normalize_header(header):\n",
    "    \"\"\"Normalizează header-ul eliminând spațiile și caracterele de nouă linie.\"\"\"\n",
    "   # header=header.rstrip('\\n')\n",
    "    # header.replace('\\n',\"\").strip()\n",
    "    for df in header.columns:\n",
    "        df=df.replace('\\n','').strip()\n",
    "    return header#header.replace('\\n','').strip()\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_folder):\n",
    "    table_titles, title_frequencies = extract_table_titles(pdf_path)\n",
    "    index=0\n",
    "    all_tables_df = pd.DataFrame()\n",
    "    # Cuvinte cheie de verificat în antetul tabelului (normalizate)\n",
    "    keywords = {\"region_id\", \"country_id\", \"location_id\", \"job_id\"}\n",
    "    \n",
    "    # Creează directorul de ieșire dacă nu există\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Creează DataFrame din tabel\n",
    "                if len(table) > 1:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                else:\n",
    "                    continue  # Sari peste tabelele fără date\n",
    "                \n",
    "                # Normalizează antetul\n",
    "                headers = [header for header in df.columns] #{normalize_header(header) for header in df.columns} # [header for header in df.columns] \n",
    "                #df=normalize_header(df)\n",
    "                #headers2=[header for header in df.columns] \n",
    "                print(f\"Page {page_num + 1}, Table {table_index + 1} headers: {headers}\")\n",
    "                \n",
    "                # Verifică dacă antetul conține toate cuvintele cheie\n",
    "                ok = 0\n",
    "                for keyword in keywords:\n",
    "                    if keyword in headers:\n",
    "                        ok = 1\n",
    "                        break\n",
    "\n",
    "                if ok == 1:\n",
    "                    # Construiește calea fișierului folosind os.path.join\n",
    "                    table_path = os.path.join(output_folder, f\"{table_titles[index]}.csv\")\n",
    "                    index=index+1\n",
    "                    aux_header=headers\n",
    "                    #df2=df\n",
    "                    df.to_csv(table_path, index=False)\n",
    "                    if all_tables_df.empty:\n",
    "                            all_tables_df = df\n",
    "                    else:\n",
    "                            all_tables_df = pd.concat([all_tables_df, df], ignore_index=True)\n",
    "                    \n",
    "                    \n",
    "                else: \n",
    "                    previous_table_path = os.path.join(output_folder, f\"{table_titles[index - 1]}.csv\")\n",
    "                    df_existent = pd.read_csv(previous_table_path)\n",
    "                    df.loc[-1] = df.columns  # Adaugă antetul inițial ca prima linie\n",
    "                    df.index = df.index + 1  # Mută toate indexurile în jos\n",
    "                    df = df.sort_index()  \n",
    "                    df.columns = aux_header\n",
    "                    df_existent = pd.concat([df_existent, df], ignore_index=True)\n",
    "                    #print(df_existent)\n",
    "                    df_existent.to_csv(previous_table_path, index=False)\n",
    "                    \n",
    "    \n",
    "\n",
    "                \n",
    "                print(f\"Saved table: {table_path}, OK: {ok}\")\n",
    "   \n",
    "    print(\"Table extraction complete.\")\n",
    "    \n",
    "# Example usage\n",
    "pdf_path = 'Employee-details-1.pdf'  # Path to your PDF file\n",
    "output_folder = 'extracted_content'  # Output folder to save images and tables\n",
    "output_folder2='extracted_images'\n",
    "import os\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "if not os.path.exists(output_folder2):\n",
    "    os.makedirs(output_folder2)\n",
    "\n",
    "# ok\n",
    "extract_images_from_pdf(pdf_path, output_folder2)\n",
    "extract_tables_from_pdf(pdf_path, output_folder)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file\n",
    "path = r\"C:/Users/Talent2/Desktop/ness/extracted_content\"\n",
    "dir_list = os.listdir(path)\n",
    "for file in dir_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        df_existent = pd.read_csv(os.path.join(path, file))\n",
    "        print(df_existent.head())\n",
    "\n",
    "\n",
    "#df_existent = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "#print(df_existent.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79adc7f1-fd33-4d40-befe-7b8b7d27d434",
   "metadata": {},
   "source": [
    "# in caz ca vrei sa iti afisezi csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc1923d9-a461-49b6-849f-4dcd75c6ec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id first_name last_name     email  phone_number    hire_date  \\\n",
      "0        100.0     Steven      King     SKING  515.123.4567  17-JUN-1987   \n",
      "1        101.0      Neena   Kochhar  NKOCHHAR  515.123.4568  21-SEP-1989   \n",
      "2        102.0        Lex   De Haan   LDEHAAN  515.123.4569  13-JAN-1993   \n",
      "3        103.0  Alexander    Hunold   AHUNOLD  590.423.4567  03-JAN-1990   \n",
      "4        104.0      Bruce     Ernst    BERNST  590.423.4568  21-MAY-1991   \n",
      "\n",
      "    job_id   salary  commission_pct  manager_id  ...  Charles  Johnson  \\\n",
      "0  AD_PRES  24000.0             NaN         NaN  ...      NaN      NaN   \n",
      "1    AD_VP  17000.0             NaN       100.0  ...      NaN      NaN   \n",
      "2    AD_VP  17000.0             NaN       100.0  ...      NaN      NaN   \n",
      "3  IT_PROG   9000.0             NaN       102.0  ...      NaN      NaN   \n",
      "4  IT_PROG   6000.0             NaN       103.0  ...      NaN      NaN   \n",
      "\n",
      "  CJOHNSON 011.44.1644.429262 04-JAN-2000 SA_REP 6200 .10  149  80  \n",
      "0      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "1      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "2      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "3      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "4      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file\n",
    "csv_path = r\"C:/Users/Talent2/Desktop/ness/extracted_content/Employees.csv\"\n",
    "df_existent = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df_existent.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19516c-eb00-4d32-b21b-8ff424fe4a8b",
   "metadata": {},
   "source": [
    "# pentru procesare de imagini\n",
    "## aici trebuie concatenat intr un string ce contine 'e' si dupa se pot face embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05fbbce7-a562-495c-919c-2bba4091f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine preprocesată salvată la: preprocessed_image.png\n",
      "Extracted Text: John Doe\n",
      "\n",
      "Executive Director\n",
      "phone number 148.284.3886\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Setarea căii către executabilul Tesseract OCR\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Modificați calea după locația instalării Tesseract\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Citește imaginea\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Verifică dacă imaginea a fost încărcată corect\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Imaginea nu a putut fi găsită la calea specificată: {image_path}\")\n",
    "    \n",
    "    # Convertirea imaginii în nuanțe de gri\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Inversarea imaginii\n",
    "    inverted = cv2.bitwise_not(gray)\n",
    "    \n",
    "    # Aplicarea unui filtru de umbrire pentru a îmbunătăți contrastul\n",
    "    _, thresholded = cv2.threshold(inverted, 150, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Salvarea imaginii preprocesate pentru verificare (opțional)\n",
    "    preprocessed_path = 'preprocessed_image.png'\n",
    "    cv2.imwrite(preprocessed_path, thresholded)\n",
    "    print(f\"Imagine preprocesată salvată la: {preprocessed_path}\")\n",
    "    \n",
    "    return thresholded\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        # Preprocesarea imaginii\n",
    "        preprocessed_image = preprocess_image(image_path)\n",
    "        \n",
    "        # Convertirea imaginii preprocesate la un format compatibil cu PIL\n",
    "        pil_image = Image.fromarray(preprocessed_image)\n",
    "        \n",
    "        # Utilizarea Tesseract pentru a extrage textul\n",
    "        text = pytesseract.image_to_string(pil_image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Eroare la extragerea textului: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Exemplu de utilizare\n",
    "image_path = r\"C:\\Users\\Talent2\\Desktop\\ness\\extracted_images\\Image1.jpg\"\n",
    "try:\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd637c-5672-43b3-b0fe-0705cf5572a4",
   "metadata": {},
   "source": [
    "# scurta verificare pentru chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "581fd87a-e677-47f2-a350-59cff5aca2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine preprocesată salvată la: preprocessed_image.png\n",
      "Answer: {'query': 'What is John Doe phone number?', 'result': \"John Doe's phone number is 148.284.3886.\"}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Modificați calea după locația instalării Tesseract\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Citește imaginea\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Verifică dacă imaginea a fost încărcată corect\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Imaginea nu a putut fi găsită la calea specificată: {image_path}\")\n",
    "    \n",
    "    # Convertirea imaginii în nuanțe de gri\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Inversarea imaginii\n",
    "    inverted = cv2.bitwise_not(gray)\n",
    "    \n",
    "    # Aplicarea unui filtru de umbrire pentru a îmbunătăți contrastul\n",
    "    _, thresholded = cv2.threshold(inverted, 150, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Salvarea imaginii preprocesate pentru verificare (opțional)\n",
    "    preprocessed_path = 'preprocessed_image.png'\n",
    "    cv2.imwrite(preprocessed_path, thresholded)\n",
    "    print(f\"Imagine preprocesată salvată la: {preprocessed_path}\")\n",
    "    \n",
    "    return thresholded\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        # Preprocesarea imaginii\n",
    "        preprocessed_image = preprocess_image(image_path)\n",
    "        \n",
    "        # Convertirea imaginii preprocesate la un format compatibil cu PIL\n",
    "        pil_image = Image.fromarray(preprocessed_image)\n",
    "        \n",
    "        # Utilizarea Tesseract pentru a extrage textul\n",
    "        text = pytesseract.image_to_string(pil_image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Eroare la extragerea textului: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Exemplu de utilizare\n",
    "image_path = r\"C:\\Users\\Talent2\\Desktop\\ness\\extracted_images\\Image1.jpg\"\n",
    "try:\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    #print(\"Extracted Text:\", extracted_text)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "documents = [Document(page_content=extracted_text, metadata={\"source\": \"Image1.jpg\"})]\n",
    "\n",
    "# Load CSV files using LangChain's CSVLoader\n",
    "path = r\"extracted_content\"\n",
    "\n",
    "\n",
    "# Create OpenAIEmbeddings instance\n",
    "api_key = os.getenv('OPENAI_API_KEY')  # Replace with your OpenAI API key\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "# ChromaDB setup\n",
    "persist_directory = 'chroma_db'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()  # Persist the vector database to disk\n",
    "\n",
    "# RAG setup\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n",
    "\n",
    "# Example query\n",
    "query = \"What is John Doe phone number?\"\n",
    "answer = qa.invoke(query)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c5136-f86e-4e70-88a9-886a23ce4fa9",
   "metadata": {},
   "source": [
    "# build relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a648d891-7434-4e2e-a1d5-35e3c2ea8d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine preprocesată salvată la: preprocessed_image.png\n",
      "Answer: {'query': 'What is John Doe phone number?', 'result': \"John Doe's phone number is 515.124.4269.\"}\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Modificați calea după locația instalării Tesseract\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Citește imaginea\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Verifică dacă imaginea a fost încărcată corect\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Imaginea nu a putut fi găsită la calea specificată: {image_path}\")\n",
    "    \n",
    "    # Convertirea imaginii în nuanțe de gri\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Inversarea imaginii\n",
    "    inverted = cv2.bitwise_not(gray)\n",
    "    \n",
    "    # Aplicarea unui filtru de umbrire pentru a îmbunătăți contrastul\n",
    "    _, thresholded = cv2.threshold(inverted, 150, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Salvarea imaginii preprocesate pentru verificare (opțional)\n",
    "    preprocessed_path = 'preprocessed_image.png'\n",
    "    cv2.imwrite(preprocessed_path, thresholded)\n",
    "    print(f\"Imagine preprocesată salvată la: {preprocessed_path}\")\n",
    "    \n",
    "    return thresholded\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        # Preprocesarea imaginii\n",
    "        preprocessed_image = preprocess_image(image_path)\n",
    "        \n",
    "        # Convertirea imaginii preprocesate la un format compatibil cu PIL\n",
    "        pil_image = Image.fromarray(preprocessed_image)\n",
    "        \n",
    "        # Utilizarea Tesseract pentru a extrage textul\n",
    "        text = pytesseract.image_to_string(pil_image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Eroare la extragerea textului: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Exemplu de utilizare\n",
    "image_path = r\"extracted_images\\Image1.jpg\"\n",
    "try:\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    extracted_t=\"These are the only informations about John Doe: \" + extracted_text\n",
    "    #print(\"Extracted Text:\", extracted_text)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "documents = [Document(page_content=extracted_t, metadata={\"source\": \"Image1.jpg\"})]\n",
    "\n",
    "path = r\"C:/Users/Talent2/Desktop/ness/extracted_content\"\n",
    "countries_df = pd.read_csv(os.path.join(path, 'Countries.csv'))\n",
    "departments_df = pd.read_csv(os.path.join(path, 'Departments.csv'))\n",
    "employees_df = pd.read_csv(os.path.join(path, 'Employees.csv'))\n",
    "jobs_df = pd.read_csv(os.path.join(path, 'Jobs.csv'))\n",
    "locations_df = pd.read_csv(os.path.join(path, 'Locations.csv'))\n",
    "regions_df = pd.read_csv(os.path.join(path, 'Regions.csv'))\n",
    "p1 = r\"extracted_content/Countries.csv\"\n",
    "p2 = r\"extracted_content/Departments.csv\"\n",
    "p3 = r\"extracted_content/Employees.csv\"\n",
    "p4 = r\"extracted_content/Jobs.csv\"\n",
    "p5 = r\"extracted_content/Locations.csv\"\n",
    "p6 = r\"extracted_content/Regions.csv\"\n",
    "merged_df = pd.merge(regions_df, countries_df, on='region_id')\n",
    "merged_df = pd.merge(merged_df, locations_df, on='country_id')\n",
    "merged_df = pd.merge(merged_df, departments_df, on='location_id')\n",
    "merged_df = pd.merge(merged_df, employees_df, on='department_id')\n",
    "merged_df = pd.merge(merged_df, employees_df, on='job_id')\n",
    "\n",
    "\n",
    "# Print columns in merged_df to verify 'text' column existence\n",
    "#print(\"Columns in merged_df:\", merged_df.columns)\n",
    "\n",
    "# Access 'text' column\n",
    "merged_df['text'] = merged_df.astype(str).apply(' '.join, axis=1)\n",
    "text_column = merged_df['text']\n",
    "\n",
    "loader = DataFrameLoader(data_frame=merged_df, page_content_column='text')\n",
    "documents = documents + loader.load()\n",
    "\n",
    "# Create OpenAIEmbeddings instance\n",
    "api_key = os.getenv('OPENAI_API_KEY')  # Replace with your OpenAI API key\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "# ChromaDB setup\n",
    "persist_directory = 'chroma_db'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()  # Persist the vector database to disk\n",
    "\n",
    "# RAG setup\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n",
    "\n",
    "# Example query\n",
    "query = \"What is John Doe phone number?\"\n",
    "try:\n",
    "    answer = qa.invoke(query)\n",
    "    print(\"Answer:\", answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during query execution: {e}\")\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4284a70-b6b0-4e5a-bcb4-1ad5eee2db23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
