{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b17d220-68c1-4a97-96c1-bfdcc482544d",
   "metadata": {},
   "source": [
    "# csv transformare"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 79,
=======
   "execution_count": 17,
>>>>>>> master
   "id": "a84e6225-d1d3-4fc0-ba8d-2352a058c58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: extracted_images\\Image1.jpg\n",
      "Image extraction complete.\n",
      "Page 1, Table 1 headers: ['region_id', 'region_name']\n",
      "Saved table: extracted_content\\Regions.csv, OK: 1\n",
      "Page 1, Table 2 headers: ['country_id', 'country_name', 'region_id']\n",
      "Saved table: extracted_content\\Countries.csv, OK: 1\n",
      "Page 2, Table 1 headers: ['location_id', 'street_address', 'postal_code', 'city', 'state_province', 'country_id']\n",
      "Saved table: extracted_content\\Locations.csv, OK: 1\n",
      "Page 3, Table 1 headers: ['department_id', 'department_name', 'manager_id', 'location_id']\n",
      "Saved table: extracted_content\\Departments.csv, OK: 1\n",
      "Page 4, Table 1 headers: ['job_id', 'job_title', 'min_salary', 'max_salary']\n",
      "Saved table: extracted_content\\Jobs.csv, OK: 1\n",
      "Page 5, Table 1 headers: ['employee_id', 'first_name', 'last_name', 'email', 'phone_number', 'hire_date', 'job_id', 'salary', 'commission_pct', 'manager_id', 'department_id']\n",
      "Saved table: extracted_content\\Employees.csv, OK: 1\n",
      "Page 6, Table 1 headers: ['132', 'TJ', 'Olson', 'TJOLSON', '650.124.8234', '10-APR-1999', 'ST_CLERK', '2100', 'NULL', '121', '50']\n",
      "Saved table: extracted_content\\Employees.csv, OK: 0\n",
      "Page 7, Table 1 headers: ['179', 'Charles', 'Johnson', 'CJOHNSON', '011.44.1644.429262', '04-JAN-2000', 'SA_REP', '6200', '.10', '149', '80']\n",
      "Saved table: extracted_content\\Employees.csv, OK: 0\n",
      "Table extraction complete.\n",
      "  country_id              country_name  region_id\n",
      "0         IT                     Italy          1\n",
      "1         JP                     Japan          3\n",
      "2         US  United States of America          2\n",
      "3         CA                    Canada          2\n",
      "4         CN                     China          3\n",
      "   department_id  department_name  manager_id  location_id\n",
      "0             10   Administration       200.0         1700\n",
      "1             20        Marketing       201.0         1800\n",
      "2             30       Purchasing       114.0         1700\n",
      "3             40  Human Resources       203.0         2400\n",
      "4             50         Shipping       121.0         1500\n",
      "   employee_id first_name last_name     email  phone_number    hire_date  \\\n",
      "0          100     Steven      King     SKING  515.123.4567  17-JUN-1987   \n",
      "1          101      Neena   Kochhar  NKOCHHAR  515.123.4568  21-SEP-1989   \n",
      "2          102        Lex   De Haan   LDEHAAN  515.123.4569  13-JAN-1993   \n",
      "3          103  Alexander    Hunold   AHUNOLD  590.423.4567  03-JAN-1990   \n",
      "4          104      Bruce     Ernst    BERNST  590.423.4568  21-MAY-1991   \n",
      "\n",
      "    job_id  salary  commission_pct  manager_id  department_id  \n",
      "0  AD_PRES   24000             NaN         NaN           90.0  \n",
      "1    AD_VP   17000             NaN       100.0           90.0  \n",
      "2    AD_VP   17000             NaN       100.0           90.0  \n",
      "3  IT_PROG    9000             NaN       102.0           60.0  \n",
      "4  IT_PROG    6000             NaN       103.0           60.0  \n",
      "       job_id                      job_title  min_salary  max_salary\n",
      "0     AD_PRES                      President       20000       40000\n",
      "1       AD_VP  Administration Vice President       15000       30000\n",
      "2     AD_ASST       Administration Assistant        3000        6000\n",
      "3      FI_MGR                Finance Manager        8200       16000\n",
      "4  FI_ACCOUNT                     Accountant        4200        9000\n",
      "   location_id           street_address postal_code       city  \\\n",
      "0         1000     1297 Via Cola di Rie       00989       Roma   \n",
      "1         1100  93091 Calle della Testa       10934     Venice   \n",
      "2         1200         2017 Shinjuku-ku        1689      Tokyo   \n",
      "3         1300          9450 Kamiya-cho        6823  Hiroshima   \n",
      "4         1400      2014 Jabberwocky Rd       26192  Southlake   \n",
      "\n",
      "     state_province country_id  \n",
      "0               NaN         IT  \n",
      "1               NaN         IT  \n",
      "2  Tokyo Prefecture         JP  \n",
      "3               NaN         JP  \n",
      "4             Texas         US  \n",
      "   region_id             region_name\n",
      "0          1                  Europe\n",
      "1          2                Americas\n",
      "2          3                    Asia\n",
      "3          4  Middle East and Africa\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def extract_table_titles(pdf_path):\n",
    "    # Deschide PDF-ul\n",
    "    doc = fitz.open(pdf_path)\n",
    "    table_titles = []\n",
    "    title_frequencies = defaultdict(int)\n",
    "\n",
    "    # Variabilă pentru a ține evidența rândurilor goale între titlurile de tabele\n",
    "    blank_lines_count = 0\n",
    "    \n",
    "    # Parcurge fiecare pagină\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            \n",
    "            # Verifică dacă linia este goală\n",
    "            if len(words) == 0:\n",
    "                blank_lines_count += 1\n",
    "            else:\n",
    "                # Verifică dacă linia conține un singur cuvânt care începe cu literă mare\n",
    "                if len(words) == 1 and words[0][0].isupper():\n",
    "                    # Pentru primul titlu de tabel, nu este nevoie să verificăm numărul de rânduri goale\n",
    "                    if not table_titles or blank_lines_count >= 2:\n",
    "                        table_titles.append(words[0])\n",
    "                        title_frequencies[words[0]] = 0\n",
    "                    blank_lines_count = 0  # Resetează contorul de rânduri goale\n",
    "                else:\n",
    "                    # Resetează contorul de rânduri goale dacă întâlnește o linie care nu este goală sau nu este titlu de tabel\n",
    "                    blank_lines_count = 0\n",
    "\n",
    "    return table_titles, dict(title_frequencies)\n",
    "    \n",
    "def extract_images_from_pdf(pdf_path, output_folder):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    index=0\n",
    "    \n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        image_list = page.get_images(full=True)\n",
    "\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image_path = os.path.join(output_folder,f\"Image{index + 1}.jpg\")\n",
    "            index=index+1\n",
    "            image.save(image_path)\n",
    "\n",
    "            print(f\"Saved image: {image_path}\")\n",
    "            \n",
    "\n",
    "    print(\"Image extraction complete.\")\n",
    "\n",
    "\n",
    "def normalize_header(header):\n",
    "    \"\"\"Normalizează header-ul eliminând spațiile și caracterele de nouă linie.\"\"\"\n",
    "   # header=header.rstrip('\\n')\n",
    "    # header.replace('\\n',\"\").strip()\n",
    "    for df in header.columns:\n",
    "        df=df.replace('\\n','').strip()\n",
    "    return header#header.replace('\\n','').strip()\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_folder):\n",
    "    table_titles, title_frequencies = extract_table_titles(pdf_path)\n",
    "    index=0\n",
    "    all_tables_df = pd.DataFrame()\n",
    "    # Cuvinte cheie de verificat în antetul tabelului (normalizate)\n",
    "    keywords = {\"region_id\", \"country_id\", \"location_id\", \"job_id\"}\n",
    "    \n",
    "    # Creează directorul de ieșire dacă nu există\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Creează DataFrame din tabel\n",
    "                if len(table) > 1:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                else:\n",
    "                    continue  # Sari peste tabelele fără date\n",
    "                \n",
    "                # Normalizează antetul\n",
    "                headers = [header for header in df.columns] #{normalize_header(header) for header in df.columns} # [header for header in df.columns] \n",
    "                #df=normalize_header(df)\n",
    "                #headers2=[header for header in df.columns] \n",
    "                print(f\"Page {page_num + 1}, Table {table_index + 1} headers: {headers}\")\n",
    "                \n",
    "                # Verifică dacă antetul conține toate cuvintele cheie\n",
    "                ok = 0\n",
    "                for keyword in keywords:\n",
    "                    if keyword in headers:\n",
    "                        ok = 1\n",
    "                        break\n",
    "\n",
    "                if ok == 1:\n",
    "                    # Construiește calea fișierului folosind os.path.join\n",
    "                    table_path = os.path.join(output_folder, f\"{table_titles[index]}.csv\")\n",
    "                    index=index+1\n",
    "                    aux_header=headers\n",
    "                    #df2=df\n",
    "                    df.to_csv(table_path, index=False)\n",
    "                    if all_tables_df.empty:\n",
    "                            all_tables_df = df\n",
    "                    else:\n",
    "                            all_tables_df = pd.concat([all_tables_df, df], ignore_index=True)\n",
    "                    \n",
    "                    \n",
    "                else: \n",
    "                    previous_table_path = os.path.join(output_folder, f\"{table_titles[index - 1]}.csv\")\n",
    "                    df_existent = pd.read_csv(previous_table_path)\n",
    "                    df.loc[-1] = df.columns  # Adaugă antetul inițial ca prima linie\n",
    "                    df.index = df.index + 1  # Mută toate indexurile în jos\n",
    "                    df = df.sort_index()  \n",
    "                    df.columns = aux_header\n",
    "                    df_existent = pd.concat([df_existent, df], ignore_index=True)\n",
    "                    #print(df_existent)\n",
    "                    df_existent.to_csv(previous_table_path, index=False)\n",
    "                    \n",
    "    \n",
    "\n",
    "                \n",
    "                print(f\"Saved table: {table_path}, OK: {ok}\")\n",
    "   \n",
    "    print(\"Table extraction complete.\")\n",
    "    \n",
    "# Example usage\n",
    "pdf_path = 'employee_details.pdf'  # Path to your PDF file\n",
    "output_folder = 'extracted_content'  # Output folder to save images and tables\n",
    "output_folder2='extracted_images'\n",
    "import os\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "if not os.path.exists(output_folder2):\n",
    "    os.makedirs(output_folder2)\n",
    "\n",
    "# ok\n",
    "extract_images_from_pdf(pdf_path, output_folder2)\n",
    "extract_tables_from_pdf(pdf_path, output_folder)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file\n",
    "path = r\"C:/Users/Talent2/Desktop/ness/extracted_content\"\n",
    "dir_list = os.listdir(path)\n",
    "for file in dir_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        df_existent = pd.read_csv(os.path.join(path, file))\n",
    "        print(df_existent.head())\n",
    "\n",
    "\n",
    "#df_existent = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "#print(df_existent.head())\n",
    "\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "79adc7f1-fd33-4d40-befe-7b8b7d27d434",
=======
   "cell_type": "code",
   "execution_count": 25,
   "id": "45a0d85b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-HBK7j***************************************4YJs. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# ChromaDB setup\u001b[39;00m\n\u001b[0;32m     23\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 24\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(documents\u001b[38;5;241m=\u001b[39mdocs, \n\u001b[0;32m     25\u001b[0m                                  embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m     26\u001b[0m                                  persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory)\n\u001b[0;32m     27\u001b[0m vectordb\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[0;32m     28\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:790\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    789\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[0;32m    791\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    792\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    793\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    794\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    795\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m    796\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[0;32m    797\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[0;32m    798\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m    799\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    801\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:748\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[0;32m    743\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    744\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    745\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    746\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    747\u001b[0m     ):\n\u001b[1;32m--> 748\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    749\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[0;32m    750\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    751\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    752\u001b[0m         )\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:276\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:535\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    534\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_len_safe_embeddings(texts, engine\u001b[38;5;241m=\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:430\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    428\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 430\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params\n\u001b[0;32m    432\u001b[0m     )\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    434\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\openai\\resources\\embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    116\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[0;32m    117\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    118\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    119\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    120\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    121\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    122\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    123\u001b[0m     ),\n\u001b[0;32m    124\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    125\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    927\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-HBK7j***************************************4YJs. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "#loading csvs\n",
    "loader = DirectoryLoader(path=\"./extracted_content\", glob=\"*.csv\", loader_cls=CSVLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Get API access\n",
    "key = os.getenv('OPENAPI_KEY')\n",
    "embedding = OpenAIEmbeddings(api_key=key)\n",
    "\n",
    "# ChromaDB setup\n",
    "persist_directory = 'db'\n",
    "vectordb = Chroma.from_documents(documents=docs, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "vectordb.persist()\n",
    "vectordb = None    \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "555b3723-f582-4302-a439-12359e1413e3",
>>>>>>> master
   "metadata": {},
   "source": [
    "# in caz ca vrei sa iti afisezi csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc1923d9-a461-49b6-849f-4dcd75c6ec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id first_name last_name     email  phone_number    hire_date  \\\n",
      "0        100.0     Steven      King     SKING  515.123.4567  17-JUN-1987   \n",
      "1        101.0      Neena   Kochhar  NKOCHHAR  515.123.4568  21-SEP-1989   \n",
      "2        102.0        Lex   De Haan   LDEHAAN  515.123.4569  13-JAN-1993   \n",
      "3        103.0  Alexander    Hunold   AHUNOLD  590.423.4567  03-JAN-1990   \n",
      "4        104.0      Bruce     Ernst    BERNST  590.423.4568  21-MAY-1991   \n",
      "\n",
      "    job_id   salary  commission_pct  manager_id  ...  Charles  Johnson  \\\n",
      "0  AD_PRES  24000.0             NaN         NaN  ...      NaN      NaN   \n",
      "1    AD_VP  17000.0             NaN       100.0  ...      NaN      NaN   \n",
      "2    AD_VP  17000.0             NaN       100.0  ...      NaN      NaN   \n",
      "3  IT_PROG   9000.0             NaN       102.0  ...      NaN      NaN   \n",
      "4  IT_PROG   6000.0             NaN       103.0  ...      NaN      NaN   \n",
      "\n",
      "  CJOHNSON 011.44.1644.429262 04-JAN-2000 SA_REP 6200 .10  149  80  \n",
      "0      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "1      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "2      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "3      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "4      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file\n",
    "csv_path = r\"extracted_content/Employees.csv\"\n",
    "df_existent = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df_existent.head())\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "id": "5e19516c-eb00-4d32-b21b-8ff424fe4a8b",
   "metadata": {},
   "source": [
    "# pentru procesare de imagini\n",
    "## aici trebuie concatenat intr un string ce contine 'e' si dupa se pot face embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05fbbce7-a562-495c-919c-2bba4091f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine preprocesată salvată la: preprocessed_image.png\n",
      "Extracted Text: John Doe\n",
      "\n",
      "Executive Director\n",
      "phone number 148.284.3886\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Setarea căii către executabilul Tesseract OCR\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Modificați calea după locația instalării Tesseract\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Citește imaginea\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Verifică dacă imaginea a fost încărcată corect\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Imaginea nu a putut fi găsită la calea specificată: {image_path}\")\n",
    "    \n",
    "    # Convertirea imaginii în nuanțe de gri\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Inversarea imaginii\n",
    "    inverted = cv2.bitwise_not(gray)\n",
    "    \n",
    "    # Aplicarea unui filtru de umbrire pentru a îmbunătăți contrastul\n",
    "    _, thresholded = cv2.threshold(inverted, 150, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Salvarea imaginii preprocesate pentru verificare (opțional)\n",
    "    preprocessed_path = 'preprocessed_image.png'\n",
    "    cv2.imwrite(preprocessed_path, thresholded)\n",
    "    print(f\"Imagine preprocesată salvată la: {preprocessed_path}\")\n",
    "    \n",
    "    return thresholded\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        # Preprocesarea imaginii\n",
    "        preprocessed_image = preprocess_image(image_path)\n",
    "        \n",
    "        # Convertirea imaginii preprocesate la un format compatibil cu PIL\n",
    "        pil_image = Image.fromarray(preprocessed_image)\n",
    "        \n",
    "        # Utilizarea Tesseract pentru a extrage textul\n",
    "        text = pytesseract.image_to_string(pil_image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Eroare la extragerea textului: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Exemplu de utilizare\n",
    "image_path = r\"C:\\Users\\Talent2\\Desktop\\ness\\extracted_images\\Image1.jpg\"\n",
    "try:\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd637c-5672-43b3-b0fe-0705cf5572a4",
   "metadata": {},
   "source": [
    "# scurta verificare pentru chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "581fd87a-e677-47f2-a350-59cff5aca2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'query': 'Name a employee from Italy .', 'result': \"I don't have information about employees from Italy in the provided context.\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load CSV files using LangChain's CSVLoader\n",
    "path = r\"C:/Users/Talent2/Desktop/ness/extracted_content\"\n",
    "documents = []\n",
    "dir_list = os.listdir(path)\n",
    "for file in dir_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        loader = CSVLoader(file_path=os.path.join(path, file))\n",
    "        documents += loader.load()\n",
    "\n",
    "# Create OpenAIEmbeddings instance\n",
    "api_key = os.getenv('OPENAI_API_KEY')  # Replace with your OpenAI API key\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "# ChromaDB setup\n",
    "persist_directory = 'chroma_db'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()  # Persist the vector database to disk\n",
    "\n",
    "# RAG setup\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n",
    "\n",
    "# Example query\n",
    "query = \"Name a employee from Italy .\"\n",
    "answer = qa.invoke(query)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c5136-f86e-4e70-88a9-886a23ce4fa9",
   "metadata": {},
   "source": [
    "# build relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a648d891-7434-4e2e-a1d5-35e3c2ea8d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in merged_df: Index(['region_id', 'region_name', 'country_id', 'country_name', 'location_id',\n",
      "       'street_address', 'postal_code', 'city', 'state_province',\n",
      "       'department_id_x', 'department_name', 'manager_id_x', 'employee_id_x',\n",
      "       'first_name_x', 'last_name_x', 'email_x', 'phone_number_x',\n",
      "       'hire_date_x', 'job_id', 'salary_x', 'commission_pct_x', 'manager_id_y',\n",
      "       'employee_id_y', 'first_name_y', 'last_name_y', 'email_y',\n",
      "       'phone_number_y', 'hire_date_y', 'salary_y', 'commission_pct_y',\n",
      "       'manager_id', 'department_id_y'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns in merged_df:\u001b[39m\u001b[38;5;124m\"\u001b[39m, merged_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Access 'text' column\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m text_column \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_column)\n\u001b[0;32m     43\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataFrameLoader(data_frame\u001b[38;5;241m=\u001b[39mmerged_df)\n",
      "File \u001b[1;32m~\\Desktop\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\Desktop\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "path = r\"C:/Users/Talent2/Desktop/ness/extracted_content\"\n",
    "countries_df = pd.read_csv(os.path.join(path, 'Countries.csv'))\n",
    "departments_df = pd.read_csv(os.path.join(path, 'Departments.csv'))\n",
    "employees_df = pd.read_csv(os.path.join(path, 'Employees.csv'))\n",
    "jobs_df = pd.read_csv(os.path.join(path, 'Jobs.csv'))\n",
    "locations_df = pd.read_csv(os.path.join(path, 'Locations.csv'))\n",
    "regions_df = pd.read_csv(os.path.join(path, 'Regions.csv'))\n",
    "p1 = r\"C:/Users/Talent2/Desktop/ness/extracted_content/Countries.csv\"\n",
    "p2 = r\"C:/Users/Talent2/Desktop/ness/extracted_content/Departments.csv\"\n",
    "p3 = r\"C:/Users/Talent2/Desktop/ness/extracted_content/Employees.csv\"\n",
    "p4 = r\"C:/Users/Talent2/Desktop/ness/extracted_content/Jobs.csv\"\n",
    "p5 = r\"C:/Users/Talent2/Desktop/ness/extracted_content/Locations.csv\"\n",
    "p6 = r\"C:/Users/Talent2/Desktop/ness/extracted_content/Regions.csv\"\n",
    "merged_df = pd.merge(regions_df, countries_df, on='region_id')\n",
    "merged_df = pd.merge(merged_df, locations_df, on='country_id')\n",
    "merged_df = pd.merge(merged_df, departments_df, on='location_id')\n",
    "merged_df = pd.merge(merged_df, employees_df, on='department_id')\n",
    "merged_df = pd.merge(merged_df, employees_df, on='job_id')\n",
    "\n",
    "\n",
    "# Print columns in merged_df to verify 'text' column existence\n",
    "print(\"Columns in merged_df:\", merged_df.columns)\n",
    "\n",
    "# Access 'text' column\n",
    "text_column = merged_df['text']\n",
    "print(text_column)\n",
    "\n",
    "loader = DataFrameLoader(data_frame=merged_df)\n",
    "documents = loader.load()\n",
    "\n",
    "# Create OpenAIEmbeddings instance\n",
    "api_key = os.getenv('OPENAI_API_KEY')  # Replace with your OpenAI API key\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "# ChromaDB setup\n",
    "persist_directory = 'chroma_db'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()  # Persist the vector database to disk\n",
    "\n",
    "# RAG setup\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n",
    "\n",
    "# Example query\n",
    "query = \"Name a employee from Italy .\"\n",
    "answer = qa.invoke(query)\n",
    "print(\"Answer:\", answer)\n",
    "\n",
    "\n",
    "print(\"ok\")"
=======
   "cell_type": "code",
   "execution_count": 15,
   "id": "04030076-7846-4e44-a81b-ceb30c6acc45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_titles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cale_fisier_csv \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_titles[index\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Încărcați datele din fișierul CSV existent într-un DataFrame\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df_existent \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(cale_fisier_csv)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'table_titles' is not defined"
     ]
    }
   ],
   "source": [
    "cale_fisier_csv = os.path.join(output_folder, f\"{table_titles[index-1]}.csv\")\n",
    "# Încărcați datele din fișierul CSV existent într-un DataFrame\n",
    "df_existent = pd.read_csv(cale_fisier_csv)\n",
    "# Adăugați tabelul extras la DataFrame-ul existent\n",
    "df2 = pd.concat([df2, df], ignore_index=True)\n",
    "# Salvați DataFrame-ul final în fișierul CSV existent\n",
    "df_final=df2\n",
    "df_final.to_csv(cale_fisier_csv, index=False)"
>>>>>>> master
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
