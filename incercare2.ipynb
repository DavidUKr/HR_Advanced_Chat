{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b17d220-68c1-4a97-96c1-bfdcc482544d",
   "metadata": {},
   "source": [
    "# csv transformare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e6225-d1d3-4fc0-ba8d-2352a058c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def extract_table_titles(pdf_path):\n",
    "    # Deschide PDF-ul\n",
    "    doc = fitz.open(pdf_path)\n",
    "    table_titles = []\n",
    "    title_frequencies = defaultdict(int)\n",
    "\n",
    "    # Variabilă pentru a ține evidența rândurilor goale între titlurile de tabele\n",
    "    blank_lines_count = 0\n",
    "    \n",
    "    # Parcurge fiecare pagină\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            \n",
    "            # Verifică dacă linia este goală\n",
    "            if len(words) == 0:\n",
    "                blank_lines_count += 1\n",
    "            else:\n",
    "                # Verifică dacă linia conține un singur cuvânt care începe cu literă mare\n",
    "                if len(words) == 1 and words[0][0].isupper():\n",
    "                    # Pentru primul titlu de tabel, nu este nevoie să verificăm numărul de rânduri goale\n",
    "                    if not table_titles or blank_lines_count >= 2:\n",
    "                        table_titles.append(words[0])\n",
    "                        title_frequencies[words[0]] = 0\n",
    "                    blank_lines_count = 0  # Resetează contorul de rânduri goale\n",
    "                else:\n",
    "                    # Resetează contorul de rânduri goale dacă întâlnește o linie care nu este goală sau nu este titlu de tabel\n",
    "                    blank_lines_count = 0\n",
    "\n",
    "    return table_titles, dict(title_frequencies)\n",
    "    \n",
    "def extract_images_from_pdf(pdf_path, output_folder):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    index=0\n",
    "    \n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        image_list = page.get_images(full=True)\n",
    "\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image_path = os.path.join(output_folder,f\"Image{index + 1}.jpg\")\n",
    "            index=index+1\n",
    "            image.save(image_path)\n",
    "\n",
    "            print(f\"Saved image: {image_path}\")\n",
    "            \n",
    "\n",
    "    print(\"Image extraction complete.\")\n",
    "\n",
    "\n",
    "def normalize_header(header):\n",
    "    \"\"\"Normalizează header-ul eliminând spațiile și caracterele de nouă linie.\"\"\"\n",
    "   # header=header.rstrip('\\n')\n",
    "    # header.replace('\\n',\"\").strip()\n",
    "    for df in header.columns:\n",
    "        df=df.replace('\\n','').strip()\n",
    "    return header#header.replace('\\n','').strip()\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_folder):\n",
    "    table_titles, title_frequencies = extract_table_titles(pdf_path)\n",
    "    index=0\n",
    "    all_tables_df = pd.DataFrame()\n",
    "    # Cuvinte cheie de verificat în antetul tabelului (normalizate)\n",
    "    keywords = {\"region_id\", \"country_id\", \"location_id\", \"job_id\"}\n",
    "    \n",
    "    # Creează directorul de ieșire dacă nu există\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Creează DataFrame din tabel\n",
    "                if len(table) > 1:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                else:\n",
    "                    continue  # Sari peste tabelele fără date\n",
    "                \n",
    "                # Normalizează antetul\n",
    "                headers = [header for header in df.columns] #{normalize_header(header) for header in df.columns} # [header for header in df.columns] \n",
    "                #df=normalize_header(df)\n",
    "                #headers2=[header for header in df.columns] \n",
    "                print(f\"Page {page_num + 1}, Table {table_index + 1} headers: {headers}\")\n",
    "                \n",
    "                # Verifică dacă antetul conține toate cuvintele cheie\n",
    "                ok = 0\n",
    "                for keyword in keywords:\n",
    "                    if keyword in headers:\n",
    "                        ok = 1\n",
    "                        break\n",
    "\n",
    "                if ok == 1:\n",
    "                    # Construiește calea fișierului folosind os.path.join\n",
    "                    table_path = os.path.join(output_folder, f\"{table_titles[index]}.csv\")\n",
    "                    index=index+1\n",
    "                    aux_header=headers\n",
    "                    #df2=df\n",
    "                    df.to_csv(table_path, index=False)\n",
    "                    if all_tables_df.empty:\n",
    "                            all_tables_df = df\n",
    "                    else:\n",
    "                            all_tables_df = pd.concat([all_tables_df, df], ignore_index=True)\n",
    "                    \n",
    "                    \n",
    "                else: \n",
    "                    previous_table_path = os.path.join(output_folder, f\"{table_titles[index - 1]}.csv\")\n",
    "                    df_existent = pd.read_csv(previous_table_path)\n",
    "                    df.loc[-1] = df.columns  # Adaugă antetul inițial ca prima linie\n",
    "                    df.index = df.index + 1  # Mută toate indexurile în jos\n",
    "                    df = df.sort_index()  \n",
    "                    df.columns = aux_header\n",
    "                    df_existent = pd.concat([df_existent, df], ignore_index=True)\n",
    "                    #print(df_existent)\n",
    "                    df_existent.to_csv(previous_table_path, index=False)\n",
    "                    \n",
    "    \n",
    "\n",
    "                \n",
    "                print(f\"Saved table: {table_path}, OK: {ok}\")\n",
    "   \n",
    "    print(\"Table extraction complete.\")\n",
    "    \n",
    "# Example usage\n",
    "pdf_path = 'Employee-details-1.pdf'  # Path to your PDF file\n",
    "output_folder = 'extracted_content'  # Output folder to save images and tables\n",
    "output_folder2='extracted_images'\n",
    "import os\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "if not os.path.exists(output_folder2):\n",
    "    os.makedirs(output_folder2)\n",
    "\n",
    "# ok\n",
    "extract_images_from_pdf(pdf_path, output_folder2)\n",
    "extract_tables_from_pdf(pdf_path, output_folder)\n",
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file\n",
    "path = r\"C:/Users/Talent2/Desktop/ness/extracted_content\"\n",
    "dir_list = os.listdir(path)\n",
    "for file in dir_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        df_existent = pd.read_csv(os.path.join(path, file))\n",
    "        print(df_existent.head())\n",
    "\n",
    "\n",
    "#df_existent = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "#print(df_existent.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a84e6225-d1d3-4fc0-ba8d-2352a058c58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in caz ca vrei sa iti afisezi csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1923d9-a461-49b6-849f-4dcd75c6ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file\n",
    "csv_path = r\"C:/Users/Talent2/Desktop/ness/extracted_content/Employees.csv\"\n",
    "df_existent = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df_existent.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19516c-eb00-4d32-b21b-8ff424fe4a8b",
   "metadata": {},
   "source": [
    "# pentru procesare de imagini\n",
    "## aici trebuie concatenat intr un string ce contine 'e' si dupa se pot face embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbbce7-a562-495c-919c-2bba4091f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Setarea căii către executabilul Tesseract OCR\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Modificați calea după locația instalării Tesseract\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Citește imaginea\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Verifică dacă imaginea a fost încărcată corect\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Imaginea nu a putut fi găsită la calea specificată: {image_path}\")\n",
    "    \n",
    "    # Convertirea imaginii în nuanțe de gri\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Inversarea imaginii\n",
    "    inverted = cv2.bitwise_not(gray)\n",
    "    \n",
    "    # Aplicarea unui filtru de umbrire pentru a îmbunătăți contrastul\n",
    "    _, thresholded = cv2.threshold(inverted, 150, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Salvarea imaginii preprocesate pentru verificare (opțional)\n",
    "    preprocessed_path = 'preprocessed_image.png'\n",
    "    cv2.imwrite(preprocessed_path, thresholded)\n",
    "    print(f\"Imagine preprocesată salvată la: {preprocessed_path}\")\n",
    "    \n",
    "    return thresholded\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        # Preprocesarea imaginii\n",
    "        preprocessed_image = preprocess_image(image_path)\n",
    "        \n",
    "        # Convertirea imaginii preprocesate la un format compatibil cu PIL\n",
    "        pil_image = Image.fromarray(preprocessed_image)\n",
    "        \n",
    "        # Utilizarea Tesseract pentru a extrage textul\n",
    "        text = pytesseract.image_to_string(pil_image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Eroare la extragerea textului: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Exemplu de utilizare\n",
    "image_path = r\"C:\\Users\\Talent2\\Desktop\\ness\\extracted_images\\Image1.jpg\"\n",
    "try:\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd637c-5672-43b3-b0fe-0705cf5572a4",
   "metadata": {},
   "source": [
    "# scurta verificare pentru chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fd87a-e677-47f2-a350-59cff5aca2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Modificați calea după locația instalării Tesseract\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Citește imaginea\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Verifică dacă imaginea a fost încărcată corect\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Imaginea nu a putut fi găsită la calea specificată: {image_path}\")\n",
    "    \n",
    "    # Convertirea imaginii în nuanțe de gri\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Inversarea imaginii\n",
    "    inverted = cv2.bitwise_not(gray)\n",
    "    \n",
    "    # Aplicarea unui filtru de umbrire pentru a îmbunătăți contrastul\n",
    "    _, thresholded = cv2.threshold(inverted, 150, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Salvarea imaginii preprocesate pentru verificare (opțional)\n",
    "    preprocessed_path = 'preprocessed_image.png'\n",
    "    cv2.imwrite(preprocessed_path, thresholded)\n",
    "    print(f\"Imagine preprocesată salvată la: {preprocessed_path}\")\n",
    "    \n",
    "    return thresholded\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        # Preprocesarea imaginii\n",
    "        preprocessed_image = preprocess_image(image_path)\n",
    "        \n",
    "        # Convertirea imaginii preprocesate la un format compatibil cu PIL\n",
    "        pil_image = Image.fromarray(preprocessed_image)\n",
    "        \n",
    "        # Utilizarea Tesseract pentru a extrage textul\n",
    "        text = pytesseract.image_to_string(pil_image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Eroare la extragerea textului: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Exemplu de utilizare\n",
    "image_path = r\"extracted_images\\Image1.jpg\"\n",
    "try:\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    #print(\"Extracted Text:\", extracted_text)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "documents = [Document(page_content=extracted_text, metadata={\"source\": \"Image1.jpg\"})]\n",
    "\n",
    "# Load CSV files using LangChain's CSVLoader\n",
    "path = r\"extracted_content\"\n",
    "\n",
    "\n",
    "# Create OpenAIEmbeddings instance\n",
    "api_key = os.getenv('OPENAI_API_KEY')  # Replace with your OpenAI API key\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "# ChromaDB setup\n",
    "persist_directory = 'chroma_db'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()  # Persist the vector database to disk\n",
    "\n",
    "# RAG setup\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n",
    "\n",
    "# Example query\n",
    "query = \"What is John Doe phone number?\"\n",
    "answer = qa.invoke(query)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c5136-f86e-4e70-88a9-886a23ce4fa9",
   "metadata": {},
   "source": [
    "# build relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648d891-7434-4e2e-a1d5-35e3c2ea8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Modificați calea după locația instalării Tesseract\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Citește imaginea\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Verifică dacă imaginea a fost încărcată corect\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Imaginea nu a putut fi găsită la calea specificată: {image_path}\")\n",
    "    \n",
    "    # Convertirea imaginii în nuanțe de gri\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Inversarea imaginii\n",
    "    inverted = cv2.bitwise_not(gray)\n",
    "    \n",
    "    # Aplicarea unui filtru de umbrire pentru a îmbunătăți contrastul\n",
    "    _, thresholded = cv2.threshold(inverted, 150, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Salvarea imaginii preprocesate pentru verificare (opțional)\n",
    "    preprocessed_path = 'preprocessed_image.png'\n",
    "    cv2.imwrite(preprocessed_path, thresholded)\n",
    "    print(f\"Imagine preprocesată salvată la: {preprocessed_path}\")\n",
    "    \n",
    "    return thresholded\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        # Preprocesarea imaginii\n",
    "        preprocessed_image = preprocess_image(image_path)\n",
    "        \n",
    "        # Convertirea imaginii preprocesate la un format compatibil cu PIL\n",
    "        pil_image = Image.fromarray(preprocessed_image)\n",
    "        \n",
    "        # Utilizarea Tesseract pentru a extrage textul\n",
    "        text = pytesseract.image_to_string(pil_image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Eroare la extragerea textului: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Exemplu de utilizare\n",
    "image_path = r\"extracted_images\\Image1.jpg\"\n",
    "try:\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    extracted_t=\"These are the only informations about John Doe: \" + extracted_text\n",
    "    #print(\"Extracted Text:\", extracted_text)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "documents = [Document(page_content=extracted_t, metadata={\"source\": \"Image1.jpg\"})]\n",
    "\n",
    "path = r\"extracted_content\"\n",
    "countries_df = pd.read_csv(os.path.join(path, 'Countries.csv'))\n",
    "departments_df = pd.read_csv(os.path.join(path, 'Departments.csv'))\n",
    "employees_df = pd.read_csv(os.path.join(path, 'Employees.csv'))\n",
    "jobs_df = pd.read_csv(os.path.join(path, 'Jobs.csv'))\n",
    "locations_df = pd.read_csv(os.path.join(path, 'Locations.csv'))\n",
    "regions_df = pd.read_csv(os.path.join(path, 'Regions.csv'))\n",
    "p1 = r\"extracted_content/Countries.csv\"\n",
    "p2 = r\"extracted_content/Departments.csv\"\n",
    "p3 = r\"extracted_content/Employees.csv\"\n",
    "p4 = r\"extracted_content/Jobs.csv\"\n",
    "p5 = r\"extracted_content/Locations.csv\"\n",
    "p6 = r\"extracted_content/Regions.csv\"\n",
    "merged_df = pd.merge(regions_df, countries_df, on='region_id')\n",
    "merged_df = pd.merge(merged_df, locations_df, on='country_id')\n",
    "merged_df = pd.merge(merged_df, departments_df, on='location_id')\n",
    "merged_df = pd.merge(merged_df, employees_df, on='department_id')\n",
    "merged_df = pd.merge(merged_df, employees_df, on='job_id')\n",
    "\n",
    "\n",
    "# Print columns in merged_df to verify 'text' column existence\n",
    "#print(\"Columns in merged_df:\", merged_df.columns)\n",
    "\n",
    "# Access 'text' column\n",
    "merged_df['text'] = merged_df.astype(str).apply(' '.join, axis=1)\n",
    "text_column = merged_df['text']\n",
    "\n",
    "loader = DataFrameLoader(data_frame=merged_df, page_content_column='text')\n",
    "documents = documents + loader.load()\n",
    "\n",
    "# Create OpenAIEmbeddings instance\n",
    "api_key = os.getenv('OPENAI_API_KEY')  # Replace with your OpenAI API key\n",
    "embedding = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "# ChromaDB setup\n",
    "persist_directory = 'chroma_db'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "vectordb.persist()  # Persist the vector database to disk\n",
    "\n",
    "# RAG setup\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(llm, retriever=vectordb.as_retriever())\n",
    "\n",
    "# Example query\n",
    "query = \"What is John Doe phone number?\"\n",
    "try:\n",
    "    answer = qa.invoke(query)\n",
    "    print(\"Answer:\", answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during query execution: {e}\")\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import Chain\n",
    "\n",
    "class CustomChain(Chain):\n",
    "    def __init__(self, llm, csv_tool, retriever, memory):\n",
    "        self.llm = llm\n",
    "        self.csv_tool = csv_tool\n",
    "        self.retriever = retriever\n",
    "        self.memory = memory\n",
    "\n",
    "    def _call(self, inputs: dict) -> dict:\n",
    "        # Use the retriever and memory as needed\n",
    "        query = inputs.get(\"query\")\n",
    "        if query:\n",
    "            csv_result = self.csv_tool.run({\"query\": query})\n",
    "            # Combine CSV result with LLM response or handle as needed\n",
    "            llm_result = self.llm.generate(csv_result)\n",
    "            return {\"result\": llm_result}\n",
    "        return {\"result\": \"No query provided\"}\n",
    "\n",
    "    async def _acall(self, inputs: dict) -> dict:\n",
    "        raise NotImplementedError(\"CustomChain does not support async\")\n",
    "\n",
    "# Example usage\n",
    "custom_chain = CustomChain(\n",
    "    llm=llm,\n",
    "    csv_tool=csv_tool,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# Invoke the custom chain with a query\n",
    "response = custom_chain({\"query\": \"age > 30 and department == 'Engineering'\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4284a70-b6b0-4e5a-bcb4-1ad5eee2db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create your retriever and memory as usual\n",
    "retriever = vectordb.as_retriever()\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Define the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# Integrate the CSV query tool\n",
    "def query_with_csv_tool(query):\n",
    "    csv_result = csv_tool.run({\"query\": query})\n",
    "    qa_result = qa_chain.run({\"query\": csv_result})\n",
    "    return qa_result\n",
    "\n",
    "# Create a conversation chain that uses the integrated query function\n",
    "conversation_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Example invocation\n",
    "response = query_with_csv_tool(\"age > 30 and department == 'Engineering'\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
