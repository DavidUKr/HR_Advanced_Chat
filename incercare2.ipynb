{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a84e6225-d1d3-4fc0-ba8d-2352a058c58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image: Imagine.jpg\n",
      "Image extraction complete.\n",
      "\n",
      "Page 1, Table 1 headers: ['region_id', 'region_name']\n",
      "Saved table: extracted_content\\Regions.csv, OK: 1\n",
      "Page 1, Table 2 headers: ['country_id', 'country_name', 'region_id']\n",
      "Saved table: extracted_content\\Countries.csv, OK: 1\n",
      "Page 2, Table 1 headers: ['location_id', 'street_address', 'postal_code', 'city', 'state_province', 'country_id']\n",
      "Saved table: extracted_content\\Locations.csv, OK: 1\n",
      "Page 3, Table 1 headers: ['department_id', 'department_name', 'manager_id', 'location_id']\n",
      "Saved table: extracted_content\\Departments.csv, OK: 1\n",
      "Page 4, Table 1 headers: ['job_id', 'job_title', 'min_salary', 'max_salary']\n",
      "Saved table: extracted_content\\Jobs.csv, OK: 1\n",
      "Page 5, Table 1 headers: ['employee_id', 'first_name', 'last_name', 'email', 'phone_number', 'hire_date', 'job_id', 'salary', 'commission_pct', 'manager_id', 'department_id']\n",
      "Saved table: extracted_content\\Employees.csv, OK: 1\n",
      "Page 6, Table 1 headers: ['132', 'TJ', 'Olson', 'TJOLSON', '650.124.8234', '10-APR-1999', 'ST_CLERK', '2100', 'NULL', '121', '50']\n",
      "Saved table: extracted_content\\Employees.csv, OK: 0\n",
      "Page 7, Table 1 headers: ['179', 'Charles', 'Johnson', 'CJOHNSON', '011.44.1644.429262', '04-JAN-2000', 'SA_REP', '6200', '.10', '149', '80']\n",
      "Saved table: extracted_content\\Employees.csv, OK: 0\n",
      "Table extraction complete.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pdfplumber\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def extract_table_titles(pdf_path):\n",
    "    # Deschide PDF-ul\n",
    "    doc = fitz.open(pdf_path)\n",
    "    table_titles = []\n",
    "    title_frequencies = defaultdict(int)\n",
    "\n",
    "    # Variabilă pentru a ține evidența rândurilor goale între titlurile de tabele\n",
    "    blank_lines_count = 0\n",
    "    \n",
    "    # Parcurge fiecare pagină\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            \n",
    "            # Verifică dacă linia este goală\n",
    "            if len(words) == 0:\n",
    "                blank_lines_count += 1\n",
    "            else:\n",
    "                # Verifică dacă linia conține un singur cuvânt care începe cu literă mare\n",
    "                if len(words) == 1 and words[0][0].isupper():\n",
    "                    # Pentru primul titlu de tabel, nu este nevoie să verificăm numărul de rânduri goale\n",
    "                    if not table_titles or blank_lines_count >= 2:\n",
    "                        table_titles.append(words[0])\n",
    "                        title_frequencies[words[0]] = 0\n",
    "                    blank_lines_count = 0  # Resetează contorul de rânduri goale\n",
    "                else:\n",
    "                    # Resetează contorul de rânduri goale dacă întâlnește o linie care nu este goală sau nu este titlu de tabel\n",
    "                    blank_lines_count = 0\n",
    "\n",
    "    return table_titles, dict(title_frequencies)\n",
    "    \n",
    "def extract_images_from_pdf(pdf_path, output_folder):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        image_list = page.get_images(full=True)\n",
    "\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image_path = os.path.join(f\"Imagine.jpg\")\n",
    "            image.save(image_path)\n",
    "\n",
    "            print(f\"Saved image: {image_path}\")\n",
    "\n",
    "    print(\"Image extraction complete.\")\n",
    "    img = Image.open('Imagine.jpg')\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    print(text)\n",
    "\n",
    "\n",
    "def normalize_header(header):\n",
    "    \"\"\"Normalizează header-ul eliminând spațiile și caracterele de nouă linie.\"\"\"\n",
    "   # header=header.rstrip('\\n')\n",
    "    # header.replace('\\n',\"\").strip()\n",
    "    for df in header.columns:\n",
    "        df=df.replace('\\n','').strip()\n",
    "    return header#header.replace('\\n','').strip()\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path, output_folder):\n",
    "    table_titles, title_frequencies = extract_table_titles(pdf_path)\n",
    "    index=0\n",
    "    all_tables_df = pd.DataFrame()\n",
    "    # Cuvinte cheie de verificat în antetul tabelului (normalizate)\n",
    "    keywords = {\"region_id\", \"country_id\", \"location_id\", \"job_id\"}\n",
    "    \n",
    "    # Creează directorul de ieșire dacă nu există\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # Creează DataFrame din tabel\n",
    "                if len(table) > 1:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                else:\n",
    "                    continue  # Sari peste tabelele fără date\n",
    "                \n",
    "                # Normalizează antetul\n",
    "                headers = [header for header in df.columns] #{normalize_header(header) for header in df.columns} # [header for header in df.columns] \n",
    "                #df=normalize_header(df)\n",
    "                #headers2=[header for header in df.columns] \n",
    "                print(f\"Page {page_num + 1}, Table {table_index + 1} headers: {headers}\")\n",
    "                \n",
    "                # Verifică dacă antetul conține toate cuvintele cheie\n",
    "                ok = 0\n",
    "                for keyword in keywords:\n",
    "                    if keyword in headers:\n",
    "                        ok = 1\n",
    "                        break\n",
    "\n",
    "                if ok == 1:\n",
    "                    # Construiește calea fișierului folosind os.path.join\n",
    "                    table_path = os.path.join(output_folder, f\"{table_titles[index]}.csv\")\n",
    "                    index=index+1\n",
    "                    #df2=df\n",
    "                    df.to_csv(table_path, index=False)\n",
    "                    if all_tables_df.empty:\n",
    "                            all_tables_df = df\n",
    "                    else:\n",
    "                            all_tables_df = pd.concat([all_tables_df, df], ignore_index=True)\n",
    "                    \n",
    "                    \n",
    "                else: \n",
    "                    previous_table_path = os.path.join(output_folder, f\"{table_titles[index - 1]}.csv\")\n",
    "                    df_existent = pd.read_csv(previous_table_path)\n",
    "                    df_existent = pd.concat([df_existent, df], ignore_index=True)\n",
    "                    df_existent.to_csv(previous_table_path, index=False)\n",
    "                    all_tables_df = df_existent\n",
    "    \n",
    "\n",
    "                \n",
    "                print(f\"Saved table: {table_path}, OK: {ok}\")\n",
    "   \n",
    "    print(\"Table extraction complete.\")\n",
    "    \n",
    "# Example usage\n",
    "pdf_path = 'employee_details.pdf'  # Path to your PDF file\n",
    "output_folder = 'extracted_content'  # Output folder to save images and tables\n",
    "\n",
    "import os\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "# ok\n",
    "extract_images_from_pdf(pdf_path, output_folder)\n",
    "extract_tables_from_pdf(pdf_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a84e6225-d1d3-4fc0-ba8d-2352a058c58b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-HBK7j***************************************4YJs. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# ChromaDB setup\u001b[39;00m\n\u001b[0;32m     23\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 24\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(documents\u001b[38;5;241m=\u001b[39mdocs, \n\u001b[0;32m     25\u001b[0m                                  embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m     26\u001b[0m                                  persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory)\n\u001b[0;32m     27\u001b[0m vectordb\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[0;32m     28\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:790\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    789\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[0;32m    791\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    792\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m    793\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    794\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    795\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m    796\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[0;32m    797\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[0;32m    798\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[0;32m    799\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    801\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:748\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[0;32m    743\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    744\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    745\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    746\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    747\u001b[0m     ):\n\u001b[1;32m--> 748\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[0;32m    749\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[0;32m    750\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    751\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    752\u001b[0m         )\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:276\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:535\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    534\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_len_safe_embeddings(texts, engine\u001b[38;5;241m=\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:430\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    428\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 430\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params\n\u001b[0;32m    432\u001b[0m     )\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    434\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\openai\\resources\\embeddings.py:114\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    108\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    109\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    116\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[0;32m    117\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    118\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    119\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    120\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    121\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    122\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    123\u001b[0m     ),\n\u001b[0;32m    124\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    125\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    927\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-HBK7j***************************************4YJs. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "#loading csvs\n",
    "loader = DirectoryLoader(path=\"./extracted_content\", glob=\"*.csv\", loader_cls=CSVLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Get API access\n",
    "key = os.getenv('OPENAPI_KEY')\n",
    "embedding = OpenAIEmbeddings(api_key=key)\n",
    "\n",
    "# ChromaDB setup\n",
    "persist_directory = 'db'\n",
    "vectordb = Chroma.from_documents(documents=docs, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "vectordb.persist()\n",
    "vectordb = None    \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "555b3723-f582-4302-a439-12359e1413e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Set the tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Example usageemployee_details.pdf\n",
    "img = Image.open('Imagine.jpg')\n",
    "text = pytesseract.image_to_string(img)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc1923d9-a461-49b6-849f-4dcd75c6ec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   employee_id first_name last_name     email  phone_number    hire_date  \\\n",
      "0        100.0     Steven      King     SKING  515.123.4567  17-JUN-1987   \n",
      "1        101.0      Neena   Kochhar  NKOCHHAR  515.123.4568  21-SEP-1989   \n",
      "2        102.0        Lex   De Haan   LDEHAAN  515.123.4569  13-JAN-1993   \n",
      "3        103.0  Alexander    Hunold   AHUNOLD  590.423.4567  03-JAN-1990   \n",
      "4        104.0      Bruce     Ernst    BERNST  590.423.4568  21-MAY-1991   \n",
      "\n",
      "    job_id   salary  commission_pct  manager_id  ...  Charles  Johnson  \\\n",
      "0  AD_PRES  24000.0             NaN         NaN  ...      NaN      NaN   \n",
      "1    AD_VP  17000.0             NaN       100.0  ...      NaN      NaN   \n",
      "2    AD_VP  17000.0             NaN       100.0  ...      NaN      NaN   \n",
      "3  IT_PROG   9000.0             NaN       102.0  ...      NaN      NaN   \n",
      "4  IT_PROG   6000.0             NaN       103.0  ...      NaN      NaN   \n",
      "\n",
      "  CJOHNSON 011.44.1644.429262 04-JAN-2000 SA_REP 6200 .10  149  80  \n",
      "0      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "1      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "2      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "3      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "4      NaN                NaN         NaN    NaN  NaN NaN  NaN NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the provided CSV file\n",
    "csv_path = r\"extracted_content/Employees.csv\"\n",
    "df_existent = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df_existent.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04030076-7846-4e44-a81b-ceb30c6acc45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_titles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cale_fisier_csv \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_titles[index\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Încărcați datele din fișierul CSV existent într-un DataFrame\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df_existent \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(cale_fisier_csv)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'table_titles' is not defined"
     ]
    }
   ],
   "source": [
    "cale_fisier_csv = os.path.join(output_folder, f\"{table_titles[index-1]}.csv\")\n",
    "# Încărcați datele din fișierul CSV existent într-un DataFrame\n",
    "df_existent = pd.read_csv(cale_fisier_csv)\n",
    "# Adăugați tabelul extras la DataFrame-ul existent\n",
    "df2 = pd.concat([df2, df], ignore_index=True)\n",
    "# Salvați DataFrame-ul final în fișierul CSV existent\n",
    "df_final=df2\n",
    "df_final.to_csv(cale_fisier_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
